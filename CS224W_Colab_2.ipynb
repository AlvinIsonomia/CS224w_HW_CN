{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuXWJLEm2UWS"
   },
   "source": [
    "# **CS224W - Colab 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gzsP50bF6Gb"
   },
   "source": [
    "此 Colab 将使用 PyTorch Geometric (PyG) 构建自己的图神经网络，并将模型应用于两个开放图基准 (Open Graph Benchmark，OGB) 数据集。这两个数据集用于在两个不同的图相关任务上，对模型性能进行基准测试：\n",
    "- 一种是节点属性预测，预测单个节点的属性。\n",
    "- 另一种是图属性预测，预测整个图或子图。\n",
    "\n",
    "首先，我们将学习 PyTorch Geometric 如何在 PyTorch 张量中存储图形。\n",
    "\n",
    "然后，我们将使用 `ogb` 包加载并快速查看其中一个开放图谱基准 (OGB) 数据集。 OGB 是用于图机器学习的真实、大规模和多样化的基准数据集的集合。 `ogb` 包不仅提供数据集的数据加载器，还提供评估器。\n",
    "\n",
    "最后，我们将使用 PyTorch Geometric 构建我们自己的图神经网络。然后在节点属性预测和 grpah 属性预测任务上应用和评估模型。\n",
    "\n",
    "**注意**：确保**依次运行每个部分中的所有单元格**，以便中间变量/包会延续到下一个单元格\n",
    "\n",
    "在 Colab 2 上玩得开心 0v0\n",
    "\n",
    "\n",
    "In this Colab, we will construct our own graph neural network by using PyTorch Geometric (PyG) and apply the model on two of Open Graph Benchmark (OGB) datasets. Those two datasets are used to benchmark the model performance on two different graph-related tasks. One is node property prediction, predicting properties of single nodes. Another one is graph property prediction, predicting the entire graphs or subgraphs.\n",
    "\n",
    "At first, we will learn how PyTorch Geometric stores the graphs in PyTorch tensor.\n",
    "\n",
    "We will then load and take a quick look on one of the Open Graph Benchmark (OGB) datasets by using the `ogb` package. OGB is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. The `ogb` package not only provides the data loader of the dataset but also the evaluator.\n",
    "\n",
    "At last, we will build our own graph neural networks by using PyTorch Geometric. And then apply and evaluate the models on node property prediction and grpah property prediction tasks.\n",
    "\n",
    "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell\n",
    "\n",
    "Have fun on Colab 2 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGKqVEbbMEzf"
   },
   "source": [
    "# 设备\n",
    "这次运算量比较大，推荐使用GPU。\n",
    "\n",
    "# Device\n",
    "You might need to use GPU for this Colab.\n",
    "\n",
    "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0NiFL6OLpaJ"
   },
   "source": [
    "# 安装依赖库\n",
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "By2oyBw7Lrh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (1.3.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/liuchang/.local/lib/python3.8/site-packages (from ogb) (0.21.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from ogb) (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from ogb) (1.20.3)\n",
      "Requirement already satisfied: outdated>=0.2.0 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from ogb) (0.2.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from ogb) (1.7.1)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from ogb) (4.60.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from ogb) (1.26.4)\n",
      "Requirement already satisfied: littleutils in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
      "Requirement already satisfied: requests in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from outdated>=0.2.0->ogb) (2.25.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from pandas>=0.24.0->ogb) (2021.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from scikit-learn>=0.20.0->ogb) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from scikit-learn>=0.20.0->ogb) (1.0.1)\n",
      "Requirement already satisfied: typing_extensions in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from torch>=1.6.0->ogb) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from requests->outdated>=0.2.0->ogb) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/liuchang/.conda/envs/torch/lib/python3.8/site-packages (from requests->outdated>=0.2.0->ogb) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "!pip install -q torch-geometric\n",
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwwq0nSdmsOL"
   },
   "source": [
    "# 1 PyTorch Geometric (Datasets and Data)\n",
    "\n",
    "PyTorch Geometric 通常有两个用于存储图、或将图转换为 Tensor 格式的类。 一个是 `torch_geometric.datasets`，它包含各种常见的图数据集。 另一个是`torch_geometric.data`，它提供了 PyTorch Tensor 中图数据的处理。\n",
    "\n",
    "在本节中，我们将学习如何使用 `torch_geometric.datasets` 和 `torch_geometric.data`。\n",
    "\n",
    "PyTorch Geometric generally has two classes for storing or transforming the graphs into tensor format. One is the `torch_geometric.datasets`, which contains a variety of common graph datasets. Another one is `torch_geometric.data` that provides the data handling of graphs in PyTorch tensors.\n",
    "\n",
    "In this section, we will learn how to use the `torch_geometric.datasets` and `torch_geometric.data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic-o1P3r6hr2"
   },
   "source": [
    "## PyG Datasets\n",
    "`torch_geometric.datasets` 里有很多常见的图数据集，这里将通过一个样例数据集来探索 `torch_geometric.datasets` 的使用方法\n",
    "\n",
    "The `torch_geometric.datasets` has many common graph datasets. Here we will explore the usage by using one example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zT5qca3x6XpG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENZYMES(600)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "root = './enzymes'\n",
    "name = 'ENZYMES'\n",
    "\n",
    "# The ENZYMES dataset\n",
    "pyg_dataset= TUDataset('./enzymes', 'ENZYMES')\n",
    "\n",
    "# You can find that there are 600 graphs in this dataset\n",
    "# 这个数据集一共有 600 张图！\n",
    "print(pyg_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLm5vVYMAP2x"
   },
   "source": [
    "## 问题 1：ENZYMES 数据集中的类数和特征数是多少？\n",
    "\n",
    "代码参考：https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#common-benchmark-datasets\n",
    "\n",
    "\n",
    "## Question 1: What is the number of classes and number of features in the ENZYMES dataset? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8iF_Kyqr_JbY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENZYMES dataset has 6 classes\n",
      "ENZYMES dataset has 3 features\n"
     ]
    }
   ],
   "source": [
    "def get_num_classes(pyg_dataset):\n",
    "    # TODO: Implement this function that takes a PyG dataset object\n",
    "    # and return the number of classes for that dataset.\n",
    "    '''\n",
    "    TODO: 实现函数，该函数接收 PyG 数据集对象作为输入，返回数据集的类别\n",
    "    '''\n",
    "    num_classes = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~1 line of code)\n",
    "    ## Note\n",
    "    ## 1. Colab autocomplete functionality might be useful.\n",
    "    '''\n",
    "    一行代码即可，你可以试试自动补全代码会不会自己跳出来答案233\n",
    "    '''\n",
    "    num_classes = pyg_dataset.num_classes\n",
    "    #########################################\n",
    "\n",
    "    return num_classes\n",
    "\n",
    "def get_num_features(pyg_dataset):\n",
    "    # TODO: Implement this function that takes a PyG dataset object\n",
    "    # and return the number of features for that dataset.\n",
    "    '''\n",
    "    TODO：实现函数，输入是 PyG dataset 对象，返回特征的数量\n",
    "    https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Dataset.num_node_features\n",
    "    '''\n",
    "    num_features = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~1 line of code)\n",
    "    ## Note\n",
    "    ## 1. Colab autocomplete functionality might be useful.\n",
    "    num_features = pyg_dataset.num_node_features\n",
    "    #########################################\n",
    "\n",
    "    return num_features\n",
    "\n",
    "# You may find that some information need to be stored in the dataset level,\n",
    "# specifically if there are multiple graphs in the dataset\n",
    "\n",
    "num_classes = get_num_classes(pyg_dataset)\n",
    "num_features = get_num_features(pyg_dataset)\n",
    "print(\"{} dataset has {} classes\".format(name, num_classes))\n",
    "print(\"{} dataset has {} features\".format(name, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwKbzhHUAckZ"
   },
   "source": [
    "## PyG Data\n",
    "每个 PyG 数据集通常存储一个 `torch_geometric.data.Data` 对象的列表。 每个 `torch_geometric.data.Data` 对象通常代表一张图。 您可以通过对数据集进行索引来轻松获取 `Data` 对象。\n",
    "\n",
    "更多信息，比如在 `Data` 对象中都有啥，请参阅[官方文档](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data).\n",
    "\n",
    "\n",
    "Each PyG dataset usually stores a list of `torch_geometric.data.Data` objects. Each `torch_geometric.data.Data` object usually represents a graph. You can easily get the `Data` object by indexing on the dataset.\n",
    "\n",
    "For more information such as what will be stored in `Data` object, please refer to the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一张图 Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
      "节点数 37\n",
      "这是边的索引 torch.Size([2, 168])\n",
      "这是边的个数 168\n",
      "这是点的特征吗？ tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "这是图的标签 tensor([5])\n",
      "PyG无向图的边实际上会重复记录两次\n"
     ]
    }
   ],
   "source": [
    "# 大概能搞出这些~\n",
    "print(\"这是一张图\", pyg_dataset[0])\n",
    "print(\"节点数\", pyg_dataset[0].num_nodes)\n",
    "print(\"这是边的索引\", (pyg_dataset[0].edge_index).shape)\n",
    "print(\"这是边的个数\", pyg_dataset[0].num_edges)\n",
    "print(\"这是点的特征吗？\", pyg_dataset[0].x)\n",
    "print(\"这是图的标签\", pyg_dataset[0].y)\n",
    "\n",
    "for graph in pyg_dataset:\n",
    "    if graph.num_edges % 2 == 1:\n",
    "        print(\"无向图的边不会重复记录两次，没事的\")\n",
    "        break\n",
    "print(\"PyG无向图的边实际上会重复记录两次\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sCV3xJWCddX"
   },
   "source": [
    "## 问题 2：（ENZYMES 中索引为 100）图的标签是什么？\n",
    "## Question 2: What is the label of the graph (index 100 in the ENZYMES dataset)? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LIis9oTZAfs3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
      "Graph with index 100 has label 4\n"
     ]
    }
   ],
   "source": [
    "def get_graph_class(pyg_dataset, idx):\n",
    "    # TODO: Implement this function that takes a PyG dataset object,\n",
    "    # the index of the graph in dataset, and returns the class/label \n",
    "    # of the graph (in integer).\n",
    "    '''\n",
    "    实现函数，输入为 PyG dataset 对象，以及图在 dataset 中的索引\n",
    "    返回对应图的类别（一个整数）\n",
    "    '''\n",
    "    label = -1\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~1 line of code)\n",
    "    '''\n",
    "    一行代码就行，记得最后加 .item() 因为只要数字\n",
    "    '''\n",
    "    label = pyg_dataset[idx].y.item()\n",
    "    #########################################\n",
    "\n",
    "    return label\n",
    "\n",
    "# Here pyg_dataset is a dataset for graph classification\n",
    "# 这里的 pyg_dataset 是一个用于图分类的数据集\n",
    "graph_0 = pyg_dataset[0]\n",
    "print(graph_0)\n",
    "idx = 100\n",
    "label = get_graph_class(pyg_dataset, idx)\n",
    "print('Graph with index {} has label {}'.format(idx, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKhcVeAhCwoY"
   },
   "source": [
    "## 问题 3： （ENZYMES 中索引为 100）图有多少条边？\n",
    "PyG 好像默认会把无向图的一条边记两次\n",
    "## Question 3: What is the number of edges for the graph (index 200 in the ENZYMES dataset)? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "f5m2DOfhBtWv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with index 200 has 53 edges\n"
     ]
    }
   ],
   "source": [
    "def get_graph_num_edges(pyg_dataset, idx):\n",
    "    # TODO: Implement this function that takes a PyG dataset object,\n",
    "    # the index of the graph in dataset, and returns the number of \n",
    "    # edges in the graph (in integer). You should not count an edge \n",
    "    # twice if the graph is undirected. For example, in an undirected \n",
    "    # graph G, if two nodes v and u are connected by an edge, this edge\n",
    "    # should only be counted once.\n",
    "    '''\n",
    "    实现函数，接收 PyG dataset 对象和索引作为输入，返回对应图的边数（整数）\n",
    "    如果图是无向图，那么不能把每条边计算两次。（比如(u,v)(v,u)是一条边）\n",
    "    '''\n",
    "    num_edges = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. You can't return the data.num_edges directly\n",
    "    ## 2. We assume the graph is undirected\n",
    "    ## (~4 lines of code)\n",
    "    '''\n",
    "    注意，不能用 data.num_edges 直接得到，假设图是无向图\n",
    "    至多四行代码就够了\n",
    "    '''\n",
    "    num_edges = int(pyg_dataset[idx].edge_index.shape[-1] / 2)\n",
    "    #########################################\n",
    "\n",
    "    return num_edges\n",
    "\n",
    "idx = 200\n",
    "num_edges = get_graph_num_edges(pyg_dataset, idx)\n",
    "print('Graph with index {} has {} edges'.format(idx, num_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXa7yIG4E0Fp"
   },
   "source": [
    "# 2 Open Graph Benchmark (OGB)\n",
    "OGB 是用于图机器学习的真实、大规模和多样化的基准数据集的集合。\n",
    "\n",
    "`ogb` 包的数据集可以自动化下载、处理、分割并用于 `ogb` 的 Data Loader。模型的性能也可以被 `ogb` 的 Evaluator 统一评估。\n",
    "\n",
    "\n",
    "# 2 Open Graph Benchmark (OGB)\n",
    "\n",
    "The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. Its datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can also be evaluated by using the OGB Evaluator in a unified manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnazPGGAJAZN"
   },
   "source": [
    "## Dataset and Data\n",
    "OGB 也支持 PyG Dataset 和 Data。这里看一看 `ogbn-arxiv` 数据集\n",
    "\n",
    "OGB also supports the PyG dataset and data. Here we take a look on the `ogbn-arxiv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Gpc6bTm3GF02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ogbn-arxiv dataset has 1 graph\n",
      "Data(adj_t=[169343, 169343, nnz=1166243], node_year=[169343, 1], x=[169343, 128], y=[169343, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "dataset_name = 'ogbn-arxiv'\n",
    "# Load the dataset and transform it to sparse tensor\n",
    "# 加载数据集，转为稀疏张量\n",
    "dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                 transform=T.ToSparseTensor())\n",
    "print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
    "\n",
    "# Extract the graph\n",
    "# 提取图\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cw0xZJKZI-n3"
   },
   "source": [
    "## 问题 4: ogbn-arxiv 中的特征数为多少？\n",
    "## Question 4: What is the number of features in the ogbn-arxiv graph? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZP844_nT2ZJl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph has 128 features\n"
     ]
    }
   ],
   "source": [
    "def graph_num_features(data):\n",
    "    # TODO: Implement this function that takes a PyG data object,\n",
    "    # and returns the number of features in the graph (in integer).\n",
    "    '''\n",
    "    TODO: 实现函数，接收 PyG data 对象为输入，返回图中的特征量（整数）\n",
    "    '''\n",
    "    num_features = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~1 line of code)\n",
    "    num_features = data.num_features\n",
    "    #########################################\n",
    "\n",
    "    return num_features\n",
    "\n",
    "num_features = graph_num_features(data)\n",
    "print('The graph has {} features'.format(num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DP_yEQZ0NVW"
   },
   "source": [
    "# 3 GNN: 节点属性预测\n",
    "\n",
    "在本节中，我们将使用 PyTorch Geometric 构建我们的第一个图神经网络，并将其应用于节点属性预测（节点分类）。\n",
    "\n",
    "我们将使用 GCN 算子构建图神经网络 ([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)).\n",
    "\n",
    "需要直接使用 PyG 内置的 `GCNConv` 层。\n",
    "\n",
    "\n",
    "# 3 GNN: Node Property Prediction\n",
    "\n",
    "In this section we will build our first graph neural network by using PyTorch Geometric and apply it on node property prediction (node classification).\n",
    "\n",
    "We will build the graph neural network by using GCN operator ([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)).\n",
    "\n",
    "You should use the PyG built-in `GCNConv` layer directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4CcOUEoInjD"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-DCtgcHpGIpd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "# PyG 内置的 GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IK9z0wQIwzQ"
   },
   "source": [
    "## 加载、预处理数据集\n",
    "## Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "0ibJ0ieoIwQM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'ogbn-arxiv'\n",
    "dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                 transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "\n",
    "# Make the adjacency matrix to symmetric\n",
    "# 使邻接矩阵对称\n",
    "data.adj_t = data.adj_t.to_symmetric()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# If you use GPU, the device should be cuda\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "data = data.to(device)\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx['train'].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgUA815bNJ8w"
   },
   "source": [
    "## GCN Model\n",
    "现在开始实现自己的 GCN 模型吧！\n",
    "\n",
    "注意，这里作业要求使用 `torch.nn.ModuleList()` 实现模型，下面首先给出官方文档中它的用法\n",
    "\n",
    "请根据下图实现你的 `forward` 函数\n",
    "\n",
    "\n",
    "Now we will implement our GCN model!\n",
    "\n",
    "Please follow the figure below to implement your `forward` function.\n",
    "\n",
    "\n",
    "![picture](img/cs224w-colab2-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModule(\n",
       "  (linears): ModuleList(\n",
       "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (5): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (7): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (8): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (9): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linears = torch.nn.ModuleList([torch.nn.Linear(10, 10) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = self.linears[i // 2](x) + l(x)  #//是整数除法\n",
    "        return x\n",
    "\n",
    "Modulelist = MyModule()\n",
    "Modulelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "IgspXTYpNJLA"
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False):\n",
    "        # TODO: Implement this function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "        '''\n",
    "        实现 self.convs, self.bns, self.softmax 以实现初始化函数\n",
    "        '''\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        # GCNConv 层的 Module List\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        # 1 维 BN 的 Module List\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        # log SoftMax 层\n",
    "        self.softmax = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
    "        ## 2. self.convs has num_layers GCNConv layers\n",
    "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
    "        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n",
    "        ## 'out_channels'. More information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
    "        ## More information please refer to the documentation: \n",
    "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        ## (~10 lines of code)\n",
    "        '''\n",
    "        至多 10 行代码，注意\n",
    "        1. 对于 self.bns 和 self.convs 需使用 torch.nn.MoudleList\n",
    "        2. self.convs 有 num_layers 个 GCNConv layers\n",
    "        3. self.bn 有 num_layers - 1 个 BatchNorm1d 层\n",
    "        4. self.softmax 需使用 torch.nn.LogSoftmax\n",
    "        5. 你可以设置 GCNConv 的 'in_channels' 和 'out_channels' 参数，具体请参考文档：\n",
    "        https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        6. 你对于 BatchNorm1d 你只需要设置 'num_features' 参数，具体请参考文档：\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        '''\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim) )\n",
    "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "        \n",
    "        self.bns = torch.nn.ModuleList([torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)])\n",
    "        \n",
    "        self.softmax = torch.nn.LogSoftmax()\n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        # Probability of an element to be zeroed\n",
    "        # Dropout 中令元素置零的概率\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        # 跳过分类层， 直接返回节点嵌入\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        # TODO: Implement this function that takes the feature tensor x,\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "        '''\n",
    "        TODO: 实现函数，接收特征 Tensor x 和 edge_index Tensor adj_t 作为输入，\n",
    "        返回输出 Tensor\n",
    "        '''\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct the network as showing in the figure\n",
    "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
    "        ## More information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
    "        ## 3. Don't forget to set F.dropout training to self.training\n",
    "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
    "        ## (~7 lines of code)\n",
    "        '''\n",
    "        至多 7 行代码，注意：\n",
    "        1. 请如图所示构建网络\n",
    "        2. 可以用 torch.nn.functional.relu 和 torch.nn.functional.dropout \n",
    "        具体可查看官方文档 https://pytorch.org/docs/stable/nn.functional.html\n",
    "        3. 别忘了给 F.dropout 的 training 设置为 self.training\n",
    "        4. 如果 return_embeds 是 True，则跳过最后的 softmax 层\n",
    "        '''\n",
    "        for i in range(len(self.convs) - 1): # 遍历除了输出之外每一层\n",
    "            x = self.convs[i](x, adj_t)\n",
    "            \n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, self.training)\n",
    "        \n",
    "        out = self.convs[-1](x, adj_t)\n",
    "        if not self.return_embeds:\n",
    "            out = self.softmax(out)\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "FF1hnHUhO81e"
   },
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement this function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    '''\n",
    "    TODO：实现函数，使用给定的 optimizer 和 loss_fn 训练模型\n",
    "    '''\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slicing the model output and label by train_idx\n",
    "    ## 4. Feed the sliced output and label to loss_fn\n",
    "    ## (~4 lines of code)\n",
    "    '''\n",
    "    至多 4 行代码，注意\n",
    "    1. 别忘了清空 optimizer 的梯度\n",
    "    2. 把输入喂给模型\n",
    "    3. 用 train_idx 划分模型的输出和标签\n",
    "    4. 把划分好的输出与label喂给 loss_fn\n",
    "    '''\n",
    "    optimizer.zero_grad()\n",
    "    out=model(data.x,data.adj_t)\n",
    "    train_output=out[train_idx]\n",
    "    train_label=data.y[train_idx,0]\n",
    "    loss=loss_fn(train_output,train_label)\n",
    "    #########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aJdlrJQhPBsK"
   },
   "outputs": [],
   "source": [
    "# Test function here\n",
    "# 此处为测试函数\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    # TODO: Implement this function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    '''\n",
    "    TODO: 实现函数，使用给定的 split_idx 与 evaluator 测试模型\n",
    "    '''\n",
    "    # 把模型设置为测试模式\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    # 在所有数据上模型的输出\n",
    "    out = None\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~1 line of code)\n",
    "    ## Note:\n",
    "    ## 1. No index slicing here\n",
    "    '''\n",
    "    一行代码，注意不需要划分 index\n",
    "    '''\n",
    "    out=model(data.x,data.adj_t)\n",
    "    #########################################\n",
    "\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX：那么什么是 evaluator 呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Expected output format of Evaluator for ogbn-arxiv\n",
      "{'acc': acc}\n",
      "- acc (float): Accuracy score averaged across 1 task(s)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "evaluator = Evaluator(name='ogbn-arxiv')\n",
    "print(evaluator.expected_output_format) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "o7F46xkuLiOL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cuda',\n",
       " 'num_layers': 3,\n",
       " 'hidden_dim': 256,\n",
       " 'dropout': 0.5,\n",
       " 'lr': 0.01,\n",
       " 'epochs': 100}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please do not change the args\n",
    "# 这块的参数别改哦~\n",
    "args = {\n",
    "    'device': device,\n",
    "    'num_layers': 3,\n",
    "    'hidden_dim': 256,\n",
    "    'dropout': 0.5,\n",
    "    'lr': 0.01,\n",
    "    'epochs': 100,\n",
    "}\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "dT8RyM2cPGxM"
   },
   "outputs": [],
   "source": [
    "model = GCN(data.num_features, args['hidden_dim'],\n",
    "            dataset.num_classes, args['num_layers'],\n",
    "            args['dropout']).to(device)\n",
    "evaluator = Evaluator(name='ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "qd5O5cnPPdVF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-5acb09563afe>:109: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.softmax(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 4.1763, Train: 11.09%, Valid: 22.98% Test: 21.57%\n",
      "Epoch: 02, Loss: 2.2836, Train: 22.72%, Valid: 21.46% Test: 26.69%\n",
      "Epoch: 03, Loss: 1.9543, Train: 36.72%, Valid: 42.47% Test: 40.82%\n",
      "Epoch: 04, Loss: 1.7791, Train: 31.18%, Valid: 28.31% Test: 29.25%\n",
      "Epoch: 05, Loss: 1.6504, Train: 32.05%, Valid: 25.36% Test: 28.44%\n",
      "Epoch: 06, Loss: 1.5590, Train: 33.33%, Valid: 26.89% Test: 30.07%\n",
      "Epoch: 07, Loss: 1.4938, Train: 35.14%, Valid: 31.43% Test: 34.76%\n",
      "Epoch: 08, Loss: 1.4353, Train: 37.29%, Valid: 36.55% Test: 39.09%\n",
      "Epoch: 09, Loss: 1.3971, Train: 37.01%, Valid: 34.89% Test: 37.50%\n",
      "Epoch: 10, Loss: 1.3731, Train: 36.42%, Valid: 32.26% Test: 35.76%\n",
      "Epoch: 11, Loss: 1.3375, Train: 36.61%, Valid: 30.84% Test: 35.00%\n",
      "Epoch: 12, Loss: 1.3091, Train: 37.33%, Valid: 31.22% Test: 35.67%\n",
      "Epoch: 13, Loss: 1.2815, Train: 39.05%, Valid: 34.70% Test: 39.63%\n",
      "Epoch: 14, Loss: 1.2674, Train: 42.16%, Valid: 40.46% Test: 45.45%\n",
      "Epoch: 15, Loss: 1.2433, Train: 45.28%, Valid: 45.73% Test: 50.30%\n",
      "Epoch: 16, Loss: 1.2337, Train: 47.20%, Valid: 48.06% Test: 52.13%\n",
      "Epoch: 17, Loss: 1.2135, Train: 47.48%, Valid: 48.41% Test: 51.72%\n",
      "Epoch: 18, Loss: 1.2071, Train: 48.33%, Valid: 49.13% Test: 52.23%\n",
      "Epoch: 19, Loss: 1.1920, Train: 49.79%, Valid: 50.64% Test: 53.33%\n",
      "Epoch: 20, Loss: 1.1795, Train: 51.66%, Valid: 52.27% Test: 54.86%\n",
      "Epoch: 21, Loss: 1.1736, Train: 53.53%, Valid: 54.41% Test: 56.85%\n",
      "Epoch: 22, Loss: 1.1593, Train: 55.18%, Valid: 56.61% Test: 59.17%\n",
      "Epoch: 23, Loss: 1.1491, Train: 56.85%, Valid: 58.13% Test: 60.82%\n",
      "Epoch: 24, Loss: 1.1398, Train: 58.58%, Valid: 59.51% Test: 62.26%\n",
      "Epoch: 25, Loss: 1.1382, Train: 60.96%, Valid: 61.56% Test: 63.87%\n",
      "Epoch: 26, Loss: 1.1285, Train: 63.38%, Valid: 63.64% Test: 65.21%\n",
      "Epoch: 27, Loss: 1.1165, Train: 64.86%, Valid: 64.76% Test: 65.65%\n",
      "Epoch: 28, Loss: 1.1105, Train: 65.27%, Valid: 64.81% Test: 65.60%\n",
      "Epoch: 29, Loss: 1.1028, Train: 65.33%, Valid: 64.80% Test: 65.71%\n",
      "Epoch: 30, Loss: 1.0958, Train: 65.16%, Valid: 64.42% Test: 65.52%\n",
      "Epoch: 31, Loss: 1.0901, Train: 65.19%, Valid: 64.26% Test: 65.54%\n",
      "Epoch: 32, Loss: 1.0878, Train: 65.92%, Valid: 65.05% Test: 66.28%\n",
      "Epoch: 33, Loss: 1.0778, Train: 66.86%, Valid: 66.17% Test: 67.33%\n",
      "Epoch: 34, Loss: 1.0753, Train: 67.97%, Valid: 67.67% Test: 68.22%\n",
      "Epoch: 35, Loss: 1.0702, Train: 68.60%, Valid: 68.61% Test: 68.91%\n",
      "Epoch: 36, Loss: 1.0649, Train: 68.94%, Valid: 69.22% Test: 69.41%\n",
      "Epoch: 37, Loss: 1.0588, Train: 68.97%, Valid: 69.21% Test: 69.52%\n",
      "Epoch: 38, Loss: 1.0541, Train: 68.92%, Valid: 68.98% Test: 69.33%\n",
      "Epoch: 39, Loss: 1.0518, Train: 68.88%, Valid: 68.60% Test: 68.76%\n",
      "Epoch: 40, Loss: 1.0454, Train: 68.93%, Valid: 68.25% Test: 67.96%\n",
      "Epoch: 41, Loss: 1.0466, Train: 69.15%, Valid: 68.27% Test: 67.86%\n",
      "Epoch: 42, Loss: 1.0382, Train: 69.65%, Valid: 68.93% Test: 68.73%\n",
      "Epoch: 43, Loss: 1.0359, Train: 70.11%, Valid: 69.62% Test: 69.49%\n",
      "Epoch: 44, Loss: 1.0319, Train: 70.64%, Valid: 70.05% Test: 69.75%\n",
      "Epoch: 45, Loss: 1.0264, Train: 70.77%, Valid: 70.14% Test: 69.35%\n",
      "Epoch: 46, Loss: 1.0264, Train: 70.92%, Valid: 70.11% Test: 68.97%\n",
      "Epoch: 47, Loss: 1.0235, Train: 70.93%, Valid: 70.27% Test: 69.08%\n",
      "Epoch: 48, Loss: 1.0169, Train: 70.95%, Valid: 70.44% Test: 69.37%\n",
      "Epoch: 49, Loss: 1.0175, Train: 71.04%, Valid: 70.53% Test: 69.71%\n",
      "Epoch: 50, Loss: 1.0133, Train: 71.16%, Valid: 70.56% Test: 69.83%\n",
      "Epoch: 51, Loss: 1.0082, Train: 71.23%, Valid: 70.71% Test: 69.98%\n",
      "Epoch: 52, Loss: 1.0095, Train: 71.39%, Valid: 70.71% Test: 70.03%\n",
      "Epoch: 53, Loss: 1.0019, Train: 71.35%, Valid: 70.68% Test: 70.09%\n",
      "Epoch: 54, Loss: 1.0005, Train: 71.37%, Valid: 70.75% Test: 70.13%\n",
      "Epoch: 55, Loss: 1.0005, Train: 71.43%, Valid: 70.68% Test: 69.99%\n",
      "Epoch: 56, Loss: 0.9956, Train: 71.50%, Valid: 70.76% Test: 70.02%\n",
      "Epoch: 57, Loss: 0.9925, Train: 71.61%, Valid: 70.80% Test: 69.87%\n",
      "Epoch: 58, Loss: 0.9934, Train: 71.58%, Valid: 70.79% Test: 69.85%\n",
      "Epoch: 59, Loss: 0.9887, Train: 71.64%, Valid: 70.66% Test: 69.63%\n",
      "Epoch: 60, Loss: 0.9866, Train: 71.90%, Valid: 70.94% Test: 69.47%\n",
      "Epoch: 61, Loss: 0.9823, Train: 72.00%, Valid: 71.16% Test: 70.24%\n",
      "Epoch: 62, Loss: 0.9817, Train: 71.97%, Valid: 71.24% Test: 70.71%\n",
      "Epoch: 63, Loss: 0.9787, Train: 72.00%, Valid: 71.25% Test: 70.56%\n",
      "Epoch: 64, Loss: 0.9776, Train: 72.09%, Valid: 71.12% Test: 70.12%\n",
      "Epoch: 65, Loss: 0.9754, Train: 72.24%, Valid: 71.13% Test: 69.52%\n",
      "Epoch: 66, Loss: 0.9726, Train: 72.35%, Valid: 71.23% Test: 69.96%\n",
      "Epoch: 67, Loss: 0.9677, Train: 72.32%, Valid: 71.21% Test: 69.96%\n",
      "Epoch: 68, Loss: 0.9676, Train: 72.37%, Valid: 71.03% Test: 69.55%\n",
      "Epoch: 69, Loss: 0.9658, Train: 72.33%, Valid: 70.94% Test: 68.96%\n",
      "Epoch: 70, Loss: 0.9648, Train: 72.18%, Valid: 70.55% Test: 68.44%\n",
      "Epoch: 71, Loss: 0.9599, Train: 72.33%, Valid: 70.54% Test: 68.66%\n",
      "Epoch: 72, Loss: 0.9582, Train: 72.43%, Valid: 71.06% Test: 69.75%\n",
      "Epoch: 73, Loss: 0.9577, Train: 72.47%, Valid: 71.24% Test: 69.91%\n",
      "Epoch: 74, Loss: 0.9580, Train: 72.50%, Valid: 70.89% Test: 69.28%\n",
      "Epoch: 75, Loss: 0.9539, Train: 72.53%, Valid: 70.74% Test: 68.81%\n",
      "Epoch: 76, Loss: 0.9536, Train: 72.58%, Valid: 70.76% Test: 68.67%\n",
      "Epoch: 77, Loss: 0.9483, Train: 72.75%, Valid: 71.11% Test: 69.44%\n",
      "Epoch: 78, Loss: 0.9475, Train: 72.83%, Valid: 71.24% Test: 69.94%\n",
      "Epoch: 79, Loss: 0.9480, Train: 72.86%, Valid: 71.42% Test: 70.39%\n",
      "Epoch: 80, Loss: 0.9469, Train: 72.85%, Valid: 71.48% Test: 70.71%\n",
      "Epoch: 81, Loss: 0.9434, Train: 73.07%, Valid: 71.60% Test: 70.90%\n",
      "Epoch: 82, Loss: 0.9408, Train: 73.12%, Valid: 71.55% Test: 70.41%\n",
      "Epoch: 83, Loss: 0.9405, Train: 72.96%, Valid: 71.06% Test: 69.28%\n",
      "Epoch: 84, Loss: 0.9376, Train: 72.68%, Valid: 70.50% Test: 68.39%\n",
      "Epoch: 85, Loss: 0.9379, Train: 72.86%, Valid: 70.83% Test: 68.97%\n",
      "Epoch: 86, Loss: 0.9327, Train: 72.92%, Valid: 71.00% Test: 69.59%\n",
      "Epoch: 87, Loss: 0.9347, Train: 72.85%, Valid: 71.36% Test: 70.56%\n",
      "Epoch: 88, Loss: 0.9297, Train: 73.08%, Valid: 71.53% Test: 70.48%\n",
      "Epoch: 89, Loss: 0.9286, Train: 73.31%, Valid: 71.22% Test: 69.58%\n",
      "Epoch: 90, Loss: 0.9293, Train: 73.34%, Valid: 71.14% Test: 69.21%\n",
      "Epoch: 91, Loss: 0.9271, Train: 73.39%, Valid: 71.06% Test: 69.12%\n",
      "Epoch: 92, Loss: 0.9257, Train: 73.39%, Valid: 70.79% Test: 68.59%\n",
      "Epoch: 93, Loss: 0.9235, Train: 73.50%, Valid: 70.76% Test: 68.47%\n",
      "Epoch: 94, Loss: 0.9201, Train: 73.53%, Valid: 71.64% Test: 70.26%\n",
      "Epoch: 95, Loss: 0.9199, Train: 73.42%, Valid: 71.83% Test: 70.83%\n",
      "Epoch: 96, Loss: 0.9172, Train: 73.52%, Valid: 71.78% Test: 70.82%\n",
      "Epoch: 97, Loss: 0.9183, Train: 73.61%, Valid: 71.46% Test: 70.23%\n",
      "Epoch: 98, Loss: 0.9161, Train: 73.64%, Valid: 71.46% Test: 70.11%\n",
      "Epoch: 99, Loss: 0.9122, Train: 73.59%, Valid: 71.51% Test: 70.31%\n",
      "Epoch: 100, Loss: 0.9122, Train: 73.65%, Valid: 71.48% Test: 70.17%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# reset the parameters to initial random value\n",
    "# 重新初始化模型参数\n",
    "model.reset_parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "loss_fn = F.nll_loss\n",
    "\n",
    "best_model = None\n",
    "best_valid_acc = 0\n",
    "\n",
    "for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "    loss = train(model, data, train_idx, optimizer, loss_fn)\n",
    "    result = test(model, data, split_idx, evaluator)\n",
    "    train_acc, valid_acc, test_acc = result\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_acc:.2f}%, '\n",
    "          f'Valid: {100 * valid_acc:.2f}% '\n",
    "          f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "EqcextqOL2FX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Train: 73.42%, Valid: 71.83% Test: 70.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-5acb09563afe>:109: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.softmax(out)\n"
     ]
    }
   ],
   "source": [
    "best_result = test(best_model, data, split_idx, evaluator)\n",
    "train_acc, valid_acc, test_acc = best_result\n",
    "print(f'Best model: '\n",
    "      f'Train: {100 * train_acc:.2f}%, '\n",
    "      f'Valid: {100 * valid_acc:.2f}% '\n",
    "      f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duMEg-olLjbJ"
   },
   "source": [
    "## 问题 5：`best_model` 的验证、测试准确率有多好？\n",
    "\n",
    "## Question 5: What are your `best_model` validation and test accuracy? Please report them on Gradescope. For example, for an accuracy such as 50.01%, just report 50.01 and please don't include the percent sign. (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8pOD6y80TyI"
   },
   "source": [
    "# 4 GNN:图属性预测\n",
    "\n",
    "本章节将创建一个用于图属性预测的 GNN （图分类）\n",
    "\n",
    "# 4 GNN: Graph Property Prediction\n",
    "\n",
    "In this section we will create a graph neural network for graph property prediction (graph classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRg5VOEdQTa4"
   },
   "source": [
    "## 加载、预处理数据集\n",
    "\n",
    "`dataset.task_type` 可以直接查看任务类型哦！\n",
    "## Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LXb-O5QUIgTH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Task type: binary classification\n"
     ]
    }
   ],
   "source": [
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load the dataset \n",
    "dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "# Check task type\n",
    "# 这个地方很实用，可以直接查看任务的类型\n",
    "print('Task type: {}'.format(dataset.task_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7cHHbgW1c5hi"
   },
   "outputs": [],
   "source": [
    "# Load the data sets into dataloader\n",
    "# We will train the graph classification task on a batch of 32 graphs\n",
    "# Shuffle the order of graphs for training set\n",
    "'''\n",
    "给 dataloader 加载数据，我们将在 32 大小的 batch 上训练图的分类，并在训练集上 Shuffle 图\n",
    "'''\n",
    "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "AYrSnOj0Y4DK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cuda',\n",
       " 'num_layers': 5,\n",
       " 'hidden_dim': 256,\n",
       " 'dropout': 0.5,\n",
       " 'lr': 0.001,\n",
       " 'epochs': 5}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please do not change the args\n",
    "# 虽然他说不给改，但是我为了快把 epoch 从 30 改成 5 了\n",
    "args = {\n",
    "    'device': device,\n",
    "    'num_layers': 5,\n",
    "    'hidden_dim': 256,\n",
    "    'dropout': 0.5,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 5,\n",
    "}\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WLhguSTeazy"
   },
   "source": [
    "## 图预测模型\n",
    "现在我们开始实现 GCN 图预测模型！\n",
    "\n",
    "我们将复用现有的 GCN 模型以生成 `node_embeddings` 并对节点使用全局池化以预测整张图的性质。\n",
    "\n",
    "注意，下面作业需要的池化层已经在第二行中加载好了。\n",
    "\n",
    "## Graph Prediction Model\n",
    "\n",
    "Now we will implement our GCN Graph Prediction model!\n",
    "\n",
    "We will reuse the existing GCN model to generate `node_embeddings` and use  Global Pooling on the nodes to predict properties for the whole graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "3_Kq3zyjeZ22"
   },
   "outputs": [],
   "source": [
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool # 池化层已经在这加载好啦\n",
    "\n",
    "### GCN to predict graph property\n",
    "# 用于预测图属性的 GCN\n",
    "class GCN_Graph(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(GCN_Graph, self).__init__()\n",
    "\n",
    "        # Load encoders for Atoms in molecule graphs\n",
    "        # 为分子图 (molecule graphs) 中的原子 (Atom) 加载编码器\n",
    "        self.node_encoder = AtomEncoder(hidden_dim)\n",
    "\n",
    "        # Node embedding model\n",
    "        # Note that the input_dim and output_dim are set to hidden_dim\n",
    "        # 节点嵌入模型，注意，input_dim 和 output_dim 都被设置成了 hidden_dim\n",
    "        self.gnn_node = GCN(hidden_dim, hidden_dim,\n",
    "            hidden_dim, num_layers, dropout, return_embeds=True)\n",
    "\n",
    "        self.pool = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Initialize the self.pool to global mean pooling layer\n",
    "        ## More information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n",
    "        ## (~1 line of code)\n",
    "        '''\n",
    "        1 行代码：初始化 self.pool 为全局平均池化层，详情请参考官方文档\n",
    "        https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n",
    "        '''\n",
    "        #########################################\n",
    "        self.pool = global_mean_pool\n",
    "        # Output layer\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.gnn_node.reset_parameters()\n",
    "        self.linear.reset_parameters()\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        # TODO: Implement this function that takes the input tensor batched_data,\n",
    "        # returns a batched output tensor for each graph.\n",
    "        '''\n",
    "        TODO: 实现函数，接收输入 Tensor batched_data，为每个图一个 batch 的输出\n",
    "        '''\n",
    "        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "        embed = self.node_encoder(x)\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct node embeddings using existing GCN model\n",
    "        ## 2. Use global pooling layer to construct features for the whole graph\n",
    "        ## More information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n",
    "        ## 3. Use a linear layer to predict the graph property \n",
    "        ## (~3 lines of code)\n",
    "        '''\n",
    "        至多 3 行，注意：\n",
    "        1. 使用现有的 GCN 模型构建节点嵌入\n",
    "        2. 使用全局池化层构建整张图的特征，参考https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n",
    "        3. 使用一个线性层预测图的属性\n",
    "        '''\n",
    "        out = self.gnn_node(embed, edge_index)\n",
    "        out = self.pool(out, batch)\n",
    "        out = self.linear(out)\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "FJjnGuMSbjX0"
   },
   "outputs": [],
   "source": [
    "def train(model, device, data_loader, optimizer, loss_fn):\n",
    "    # TODO: Implement this function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    '''\n",
    "    TODO: 给定 optimizer 和 loss_fn 实现训练函数\n",
    "    '''\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
    "            pass\n",
    "        else:\n",
    "            ## ignore nan targets (unlabeled) when computing training loss.\n",
    "            ## 在计算训练 loss 时，忽视 nan 的目标（没标注）\n",
    "            is_labeled = batch.y == batch.y\n",
    "\n",
    "            ############# Your code here ############\n",
    "            ## Note:\n",
    "            ## 1. Zero grad the optimizer\n",
    "            ## 2. Feed the data into the model\n",
    "            ## 3. Use `is_labeled` mask to filter output and labels\n",
    "            ## 4. You might change the type of label\n",
    "            ## 5. Feed the output and label to loss_fn\n",
    "            ## (~3 lines of code)\n",
    "            '''\n",
    "            至多 3 行代码：\n",
    "            1. 清空 optimizer 的梯度\n",
    "            2. 数据喂给模型\n",
    "            3. 使用 'is_labeled' mask 过滤输出和 label\n",
    "            4. 你可能需要改变 label 的类型\n",
    "            5. 把输出和 label 喂给 loss_fn\n",
    "            '''\n",
    "            optimizer.zero_grad()\n",
    "            op=model(batch)\n",
    "            train_op=op[is_labeled]\n",
    "            #train_labels=batch.y[is_labeled]  #Warning: 答案这里用了view，我参考了下面的y.view，就把view加上了，其实我不知道具体为什么要加\n",
    "            train_labels=batch.y[is_labeled].view(-1)\n",
    "            #loss=loss_fn(train_op,train_labels)  #RuntimeError: result type Float can't be cast to the desired output type Long\n",
    "            loss=loss_fn(train_op.float(),train_labels.float())  #https://pytorch.org/docs/stable/tensors.html#torch.Tensor.float        \n",
    "            #########################################\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ztPHXq_Gzn7U"
   },
   "outputs": [],
   "source": [
    "# The evaluation function\n",
    "def eval(model, device, loader, evaluator):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        if batch.x.shape[0] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch)\n",
    "\n",
    "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
    "            y_pred.append(pred.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
    "\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "    return evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "MR1wQ4hMZeMw"
   },
   "outputs": [],
   "source": [
    "model = GCN_Graph(args['hidden_dim'],\n",
    "            dataset.num_tasks, args['num_layers'],\n",
    "            args['dropout']).to(device)\n",
    "evaluator = Evaluator(name='ogbg-molhiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "qJGTNZiuZy0A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d00b03510a44f4e856ec49a2c169d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819e1c75e3aa444b9739d3b7ce4548dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b642eaef55034aefbef54f7d7d3ceece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206211bc502d43d3b6be038ffd8ec601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.0147, Train: 71.19%, Valid: 74.73% Test: 67.07%\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bf17a2e0e84c85a9d93977cc3b9e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfdc7c010ff41f69477275ffe3eb078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29154a8928ae4502a7644dbcfb331a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5fd42597934b8697e8b370563046e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Loss: 0.8991, Train: 75.24%, Valid: 72.40% Test: 66.62%\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34491d3499ae46009ce5b309dcfd434c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cee02ac8bcb4ecaae7a2145e654ba74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e574af13f224798bc9a6e75bc625a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ee78c2d6b647dcbda9798b967b10ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Loss: 0.4985, Train: 76.54%, Valid: 73.64% Test: 73.19%\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78519eee8c2f46e7989d214a8b34f096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e5809a8b074772a9e4e0765bd94dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508160e5d7b046a1b4d1f94f936723ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f38a1a6e103471984c13cdca20c50ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Loss: 0.5842, Train: 75.62%, Valid: 73.09% Test: 72.26%\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a22cef80574490a089a0db08c30012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdc9fbeb1904e2d97ce12e95ef64a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3732d9c3480648f583f6bb70ce3ccc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e626574de3941ffbe870cc945f0c049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Loss: 0.0368, Train: 77.06%, Valid: 70.47% Test: 69.94%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model.reset_parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_model = None\n",
    "best_valid_acc = 0\n",
    "\n",
    "for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "    print('Training...')\n",
    "    loss = train(model, device, train_loader, optimizer, loss_fn)\n",
    "\n",
    "    print('Evaluating...')\n",
    "    train_result = eval(model, device, train_loader, evaluator)\n",
    "    val_result = eval(model, device, valid_loader, evaluator)\n",
    "    test_result = eval(model, device, test_loader, evaluator)\n",
    "\n",
    "    train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_acc:.2f}%, '\n",
    "          f'Valid: {100 * valid_acc:.2f}% '\n",
    "          f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Oq5QaG21dOOO"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dab3d95d57464f94ec06d0fb6fcd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e81ebc0f4c7489c957b5df882042c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b910b1871ea644baa684fe9daf3e3337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Train: 71.19%, Valid: 74.73% Test: 67.07%\n"
     ]
    }
   ],
   "source": [
    "train_acc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\n",
    "valid_acc = eval(best_model, device, valid_loader, evaluator)[dataset.eval_metric]\n",
    "test_acc = eval(best_model, device, test_loader, evaluator)[dataset.eval_metric]\n",
    "\n",
    "print(f'Best model: '\n",
    "      f'Train: {100 * train_acc:.2f}%, '\n",
    "      f'Valid: {100 * valid_acc:.2f}% '\n",
    "      f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uKs6j6t1ah3"
   },
   "source": [
    "## 问题 6: `best_model` 验证以及测试的 ROC—AUC 分数是多少？ \n",
    "\n",
    "## Question 6: What are your `best_model` validation and test ROC-AUC score? Please report them on Gradescope. For example, for an ROC-AUC score such as 50.01%, just report 50.01 and please don't include the percent sign. (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBi_t8n0iZ4P"
   },
   "source": [
    "## 问题 7 : 试试 PyG 里除了均值池化的其他池化方式吧~\n",
    "\n",
    "## Question 7 (Optional): Experiment with other two global pooling layers other than mean pooling in Pytorch Geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "taxEEWyh1_jq"
   },
   "outputs": [],
   "source": [
    "# 等我把别的写完了搞个这个好了2333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7JXsMTBgeOI"
   },
   "source": [
    "# Submission\n",
    "\n",
    "In order to get credit, you must go submit your answers on Gradescope.\n",
    "\n",
    "Also, you need to submit the `ipynb` file of Colab 2, by clicking `File` and `Download .ipynb`. Please make sure that your output of each cell is available in your `ipynb` file."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CS224W - Colab 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
